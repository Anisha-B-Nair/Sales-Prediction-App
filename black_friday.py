# -*- coding: utf-8 -*-
"""Black Friday.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I-i6rxzRVaZp_DXXgRORMTLIkO7f9obS
"""

# 1. Import Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor

from google.colab import files

# Upload your dataset files
uploaded = files.upload()

# Load the data
train = pd.read_csv("train-corrected.zip")
test = pd.read_csv("test.csv")
sample = pd.read_csv("Sample_Submission.csv")

import zipfile, os

# 1. Extract the zip
with zipfile.ZipFile("train-corrected.zip", "r") as zip_ref:
    zip_ref.extractall(".")

# check extracted files
import os
print(os.listdir("."))

# Load the extracted CSVs
train = pd.read_csv("train.csv")

print("Train Data:")
print(train.head())

print("\nMissing Values in Train:")
print(train.isnull().sum())

# Fill missing product categories with 0
train['Product_Category_2'].fillna(0, inplace=True)
train['Product_Category_3'].fillna(0, inplace=True)

test['Product_Category_2'].fillna(0, inplace=True)
test['Product_Category_3'].fillna(0, inplace=True)

# Check again for missing values
print("Missing Values after filling:")
print(train.isnull().sum())

# Check missing values
print("Missing values in train after filling:")
print(train.isnull().sum())

print("\nMissing values in test after filling:")
print(test.isnull().sum())

from sklearn.preprocessing import LabelEncoder

# Encode Gender
train['Gender'] = train['Gender'].map({'M': 1, 'F': 0})
test['Gender'] = test['Gender'].map({'M': 1, 'F': 0})

#Encode Age (ordinal)
age_map = {'0-17': 0, '18-25': 1, '26-35': 2, '36-45': 3, '46-50': 4, '51-55': 5, '55+': 6}
train['Age'] = train['Age'].map(age_map)
test['Age'] = test['Age'].map(age_map)

# One-hot encode City_Category
train = pd.get_dummies(train, columns=['City_Category'], drop_first=True)
test = pd.get_dummies(test, columns=['City_Category'], drop_first=True)

# Encode Product_Category columns as integer
prod_cat_cols = ['Product_Category_1', 'Product_Category_2', 'Product_Category_3']
for col in prod_cat_cols:
    train[col] = train[col].astype(int)
    test[col] = test[col].astype(int)

# Quick check
print("Train data after encoding:")
print(train.head())

# Convert boolean columns to int
bool_cols = ['City_Category_B', 'City_Category_C']
train[bool_cols] = train[bool_cols].astype(int)
test[bool_cols] = test[bool_cols].astype(int)

# Quick check
print(train.head())

#feature engineering
# Average purchase per user
user_avg_purchase = train.groupby('User_ID')['Purchase'].mean().reset_index()
user_avg_purchase.columns = ['User_ID', 'User_Avg_Purchase']
train = train.merge(user_avg_purchase, on='User_ID', how='left')
test = test.merge(user_avg_purchase, on='User_ID', how='left')

# Total purchase per user
user_total_purchase = train.groupby('User_ID')['Purchase'].sum().reset_index()
user_total_purchase.columns = ['User_ID', 'User_Total_Purchase']
train = train.merge(user_total_purchase, on='User_ID', how='left')
test = test.merge(user_total_purchase, on='User_ID', how='left')

# Average purchase per product
product_avg_purchase = train.groupby('Product_ID')['Purchase'].mean().reset_index()
product_avg_purchase.columns = ['Product_ID', 'Product_Avg_Purchase']
train = train.merge(product_avg_purchase, on='Product_ID', how='left')
test = test.merge(product_avg_purchase, on='Product_ID', how='left')

# Number of times user bought products
user_purchase_count = train.groupby('User_ID')['Product_ID'].count().reset_index()
user_purchase_count.columns = ['User_ID', 'User_Purchase_Count']
train = train.merge(user_purchase_count, on='User_ID', how='left')
test = test.merge(user_purchase_count, on='User_ID', how='left')

# Quick check
print(train.head())

# Ensure all feature columns are numeric
features = ['Gender','Age','Occupation','Stay_In_Current_City_Years','Marital_Status',
            'Product_Category_1','Product_Category_2','Product_Category_3',
            'City_Category_B','City_Category_C',
            'User_Avg_Purchase','User_Total_Purchase','Product_Avg_Purchase','User_Purchase_Count']

# Recreate X_train, X_test
X_train = train[features].copy()
y_train = train['Purchase'].copy()
X_test = test[features].copy()

# Convert all to numeric explicitly
for col in features:
    X_train[col] = pd.to_numeric(X_train[col], errors='coerce').fillna(0)
    X_test[col] = pd.to_numeric(X_test[col], errors='coerce').fillna(0)

# Split training data
from sklearn.model_selection import train_test_split
X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Create LightGBM dataset
import lightgbm as lgb
lgb_train = lgb.Dataset(X_tr, y_tr)
lgb_val = lgb.Dataset(X_val, y_val, reference=lgb_train)

# LightGBM parameters
params = {
    'objective': 'regression',
    'metric': 'rmse',
    'learning_rate': 0.05,
    'num_leaves': 31,
    'verbose': -1,
    'seed': 42
}

# Train with early stopping
model = lgb.train(
    params,
    lgb_train,
    num_boost_round=1000,
    valid_sets=[lgb_train, lgb_val],
    valid_names=['train','valid'],
    callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]
)

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import numpy as np

# Split data
X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Create DMatrix
dtrain = xgb.DMatrix(X_tr, label=y_tr)
dval = xgb.DMatrix(X_val, label=y_val)
dtest = xgb.DMatrix(X_test)

# Parameters
params = {
    'objective': 'reg:squarederror',
    'eval_metric': 'rmse',
    'learning_rate': 0.05,
    'max_depth': 6,
    'seed': 42
}

# Train
watchlist = [(dtrain, 'train'), (dval, 'valid')]
xgb_model = xgb.train(params, dtrain, num_boost_round=1000, early_stopping_rounds=50, evals=watchlist, verbose_eval=100)

# Predict validation
val_preds = xgb_model.predict(dval)
rmse = np.sqrt(mean_squared_error(y_val, val_preds))
print(f"XGBoost Validation RMSE: {rmse}")

# Predict test
test_preds = xgb_model.predict(dtest)

# Submission
submission = pd.DataFrame({
    'User_ID': test['User_ID'],
    'Product_ID': test['Product_ID'],
    'Purchase': test_preds
})
submission.to_csv("xgb_submission.csv", index=False)
print("XGBoost submission saved as xgb_submission.csv")

from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
import numpy as np

# Initialize Decision Tree
dt_model = DecisionTreeRegressor(
    max_depth=10,       # limit depth to prevent overfitting
    random_state=42
)

# Train on training split
dt_model.fit(X_tr, y_tr)

# Predict on validation set
dt_val_preds = dt_model.predict(X_val)
rmse = np.sqrt(mean_squared_error(y_val, dt_val_preds))
print(f"Decision Tree Validation RMSE: {rmse}")

# Predict on test set
dt_test_preds = dt_model.predict(X_test)

# Prepare submission
submission = pd.DataFrame({
    'User_ID': test['User_ID'],
    'Product_ID': test['Product_ID'],
    'Purchase': dt_test_preds
})

submission.to_csv("dt_submission.csv", index=False)
print("Decision Tree submission saved as dt_submission.csv")

import matplotlib.pyplot as plt
import lightgbm as lgb

# Plot top 15 features
lgb.plot_importance(model, max_num_features=15, importance_type='gain')
plt.title("Top 15 Features - LightGBM")
plt.show()

#Predicted vs Actual â€“ Validation Set
from sklearn.metrics import mean_squared_error
import numpy as np

# Example for LightGBM validation predictions
val_preds = model.predict(X_val, num_iteration=model.best_iteration)

plt.figure(figsize=(8,5))
plt.scatter(y_val, val_preds, alpha=0.5)
plt.xlabel("Actual Purchase")
plt.ylabel("Predicted Purchase")
plt.title("Predicted vs Actual - Validation Set")
plt.show()

# Compute RMSE
rmse = np.sqrt(mean_squared_error(y_val, val_preds))
print(f"Validation RMSE: {rmse}")

#Distribution of Predicted vs Actual Purchas
plt.figure(figsize=(8,5))
plt.hist(val_preds, bins=50, alpha=0.6, label="Predicted")
plt.hist(y_val, bins=50, alpha=0.6, label="Actual")
plt.legend()
plt.title("Purchase Distribution - Validation Set")
plt.show()

# LightGBM
lgb_test_preds = model.predict(X_test, num_iteration=model.best_iteration)

# XGBoost
xgb_test_preds = xgb_model.predict(dtest)

# Decision Tree
dt_test_preds = dt_model.predict(X_test)

# Ensemble (simple average)
ensemble_preds = (lgb_test_preds + xgb_test_preds + dt_test_preds) / 3

# Prepare submission
submission = pd.DataFrame({
    'User_ID': test['User_ID'],
    'Product_ID': test['Product_ID'],
    'Purchase': ensemble_preds
})

submission.to_csv("ensemble_submission.csv", index=False)
print("Ensemble submission saved as ensemble_submission.csv")

submission.head()

# Check shape
print(submission.shape)

# Check for negative values
print((submission['Purchase'] < 0).sum())

# Check for duplicates
print(submission.duplicated(subset=['User_ID','Product_ID']).sum())

submission['Purchase'] = submission['Purchase'].clip(lower=0)

submission.to_csv("ensemble_submission.csv", index=False)
print("Cleaned ensemble submission saved!")